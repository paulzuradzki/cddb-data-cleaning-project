{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning CDDB\n",
    "\n",
    "This notebook is an annotated walkthrough of cleaning the Compact Disc Database (CDDB) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "import clean_cddb\n",
    "from clean_cddb.utils import get_failure_cases_summary_as_formatted_table, get_check_func_descriptions\n",
    "\n",
    "def df_to_var(df, var_name):\n",
    "    globals()[var_name] = df\n",
    "    return df\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(process)d - %(levelname)s - %(message)s\",\n",
    ")\n",
    "filepath = \"../data/input/cddb.tsv\"\n",
    "source_df = pd.read_csv(filepath, sep=\"\\t\", dtype=\"str\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply validation checks (pandera schema) and review failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    validated_df = clean_cddb.schema(source_df, lazy=True)\n",
    "    logging.info(\"Validation success. No failure cases detected.\")\n",
    "except pa.errors.SchemaErrors as err:\n",
    "    logging.info(\"Validation failure. Failure cases detected.\")\n",
    "    logging.debug(err)\n",
    "    failure_cases_df = err.failure_cases\n",
    "\n",
    "failure_cases_df = failure_cases_df.pipe(get_check_func_descriptions, clean_cddb.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`failure_cases_df`\n",
    "* The `failure_cases_df` shows the name of the column, the check, failure case (example), and row index position of the failure case in the original data frame. \n",
    "* The index can support bulk operations such as joining and querying the original dataframe for failure cases or rejecting rows in the set of failure case indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    failure_cases_df\n",
    "    .loc[\n",
    "        :, [\"schema_context\", \"column\", \"check\", \"failure_case\", \"index\"]\n",
    "    ]\n",
    "    .sample(10, random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of failure cases\n",
    "\n",
    "Here we see aggregated counts of the number of failure cases for each validation check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_cases_summary = (\n",
    "    failure_cases_df.groupby([\"column\", \"check\"], as_index=False)\n",
    "    .size()\n",
    "    .sort_values(by=[\"column\", \"check\"])\n",
    "    .rename(columns={\"size\": \"counts\"})\n",
    ")\n",
    "\n",
    "failure_cases_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a helper utility function to display the source code along side each check function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report summary counts\n",
    "failure_cases_summary_table = get_failure_cases_summary_as_formatted_table(failure_cases_df)\n",
    "\n",
    "print(failure_cases_summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning step\n",
    "\n",
    "We can use the same checks from the pandera validation schema to trigger cleaning actions such as:\n",
    "* do nothing / ignore the value\n",
    "* transform the value; e.g., replace value with a substitute (e.g., 'N/A')\n",
    "* or reject the entire record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply several cleaning functions on the source_df via .pipe(Callable).\n",
    "* Each function takes a dataframe and returns a dataframe, so we can chain together the cleaning operations like so.\n",
    "* Later, we will \n",
    "  1. compare `source_df` and `clean_df` as a before/after check\n",
    "  2. re-apply our validation checks (pandera schema) to the new `clean_df` to verify that our transformations improved our data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_cddb.utils import log_df_change\n",
    "\n",
    "clean_df = (\n",
    "source_df\n",
    "\n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_standardize_various_artists)\n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_try_to_fix_encoding_errors' procedure\")\n",
    "        \n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_try_to_fix_encoding_errors, \"artist\")\n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_try_to_fix_encoding_errors' procedure\")\n",
    "    .pipe(df_to_var, 'clean_df_artist_transforms_only')\n",
    "    \n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_invalid_symbols)\n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_invalid_symbols' procedure\")\n",
    "    \n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_invalid_categories)\n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_invalid_categories' procedure\")\n",
    "    \n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_id_zero_padding)\n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_id_zero_padding' procedure\")\n",
    "    \n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_genre_invalid) \n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_genre_invalid' procedure\")\n",
    "        \n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_year)\n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_year' procedure\")\n",
    "    \n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_title)\n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_title' procedure\")\n",
    "    \n",
    "    .pipe(df_to_var, '_df_before').pipe(clean_cddb.clean_df_genre_coalesce_with_category)\n",
    "    .pipe(log_df_change, before_df=_df_before, operation_label=\"Cleaning with 'clean_cddb.clean_df_genre_coalesce_with_category' procedure\")\n",
    "    \n",
    "    # Save an intermediate dataframe prior to dropping records\n",
    "    # so we can compare with source_df later\n",
    "    .pipe(df_to_var, 'clean_df_before_drops')\n",
    "    \n",
    "    # Drop rows with \"REJECT_ROW*\" prefix\n",
    "    .query(\"~id.str.contains('REJECT_ROW')\")    \n",
    "    .drop(columns=['merged_values'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting clean up on 'artist' field with (1) standardization to \"Various\" and (2) fixing encoding issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df_sample_markdown: str = (source_df\n",
    "                                 .compare(clean_df_artist_transforms_only, result_names=('before', 'after'))\n",
    "                                 .sample(5, random_state=0)\n",
    "                                 .to_markdown()\n",
    "                                 )\n",
    "\n",
    "print(\"Example diffs between before-and-after\")\n",
    "print(comps_df_sample_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply schema to clean_df\n",
    "try:\n",
    "    validated_df = clean_cddb.schema(clean_df, lazy=True)\n",
    "    logging.info(\"Validation success. No failure cases detected.\")\n",
    "except pa.errors.SchemaErrors as err:\n",
    "    logging.info(\"Validation failure. Failure cases detected.\")\n",
    "    logging.debug(err)\n",
    "    after_cleaning_failure_cases_df = err.failure_cases\n",
    "\n",
    "after_cleaning_failure_cases_df = after_cleaning_failure_cases_df.pipe(\n",
    "    get_check_func_descriptions, clean_cddb.schema\n",
    ")\n",
    "\n",
    "after_cleaning_failure_cases_summary = (\n",
    "    after_cleaning_failure_cases_df.groupby([\"column\", \"check\"], as_index=False)\n",
    "    .size()\n",
    "    .sort_values(by=[\"column\", \"check\"])\n",
    "    .rename(columns={\"size\": \"counts\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Counts of Failure Cases Before vs After Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "failure_cases_summary\n",
    ".merge(after_cleaning_failure_cases_summary, \n",
    "       on=['column', 'check'], \n",
    "       how='outer', \n",
    "       suffixes=['_before_cleaning', '_after_cleaning']\n",
    "       )\n",
    ".fillna('')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing `source_df` and `clean_df`\n",
    "\n",
    "* We will actually use an intermediate dataframe `clean_df_before_drops` that has the same dimensions as our original dataframe.\n",
    "    * Prior to dropping dirty records, `clean_df_before_drops` has values over-written with a prefix \"REJECT_RECORD\".\n",
    "    * This enables easier side by side comparison.\n",
    "* The final output `clean_df` will actually omit records that we intend to drop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_before_drops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "comps_df = (\n",
    "source_df.sort_index()\n",
    " .compare(clean_df_before_drops.sort_index(), result_names=('before_cleaning', 'after_cleaning'))\n",
    " .astype('object')\n",
    " .fillna('')\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show before/after comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "columns_to_compare = ['artist', 'category', 'genre', 'title', 'tracks', 'year', 'id']\n",
    "\n",
    "comps_df_formatted = (\n",
    "comps_df\n",
    " .astype(str) \n",
    " .stack()\n",
    " .reset_index()\n",
    " .rename(columns={'level_0': 'row_id', 'level_1': 'before_or_after'})\n",
    " .drop(columns=['merged_values'])\n",
    " .groupby(['row_id'], as_index=False)\n",
    "    [columns_to_compare]\n",
    "    .agg(lambda row: '  =>  '.join(row))\n",
    " .replace('^(  =>  )$', '', regex=True)\n",
    ")\n",
    "\n",
    "comps_df_formatted.sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows changed per column\")\n",
    "\n",
    "for column in clean_df.columns:\n",
    "    n_rows_changed = comps_df[column][comps_df[column]['before_cleaning'] != comps_df[column]['after_cleaning']].shape[0]\n",
    "    print(f\"{column:.<15}{n_rows_changed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample transformations\n",
    "\n",
    "* Here we can see that we transform \"Various Artists\" and \"<various>\" to \"Various\". \n",
    "* We also fixed invalid characters converting text from \"JÃ¶rg Hilbert & Felix Janosa\" to \"Jörg Hilbert & Felix Janosa\".\n",
    "* Later, we will do a more comprehensive before/after analysis after applying all the cleaning transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idxs = [7629, 1822, 117, 4129]\n",
    "source_df.compare(clean_df_artist_transforms_only).loc[example_idxs, :].fillna(pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_df.sample(5, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller before/after example\n",
    "* Here we can see that we transform \"Various Artists\" and \"<various>\" to \"Various\". \n",
    "* We also fixed invalid characters converting text from \"JÃ¶rg Hilbert & Felix Janosa\" to \"Jörg Hilbert & Felix Janosa\".\n",
    "* Later, we will do a more comprehensive before/after analysis after applying all the cleaning transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idxs = [7629, 1822, 117, 4129]\n",
    "source_df.compare(clean_df_artist_transforms_only).loc[example_idxs, :].fillna(pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample transformations\n",
    "\n",
    "* Here we can see that we transform \"Various Artists\" and \"<various>\" to \"Various\". \n",
    "* We also fixed invalid characters converting text from \"JÃ¶rg Hilbert & Felix Janosa\" to \"Jörg Hilbert & Felix Janosa\".\n",
    "* Later, we will do a more comprehensive before/after analysis after applying all the cleaning transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idxs = [7629, 1822, 117, 4129]\n",
    "source_df.compare(clean_df_artist_transforms_only).loc[example_idxs, :].fillna(pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform to track-level data\n",
    "\n",
    "We want a separate track-level dataset that can be joined to the album-level data in `clean_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to track-level data\n",
    "track_level_df = (\n",
    "    # Start with original dataframe\n",
    "    clean_df\n",
    "    # Split 'tracks' on pipe into an array; we can \"explode\" it later\n",
    "    .pipe(lambda _df: _df.assign(tracks=_df[\"tracks\"].str.split(\"|\")))\n",
    "    # \"explode\"/expand from \"tracks\" array in to 1 observation per track\n",
    "    # perform a self-join to CD data set; the CD-level data will repeat for each track\n",
    "    .pipe(\n",
    "        lambda _df: _df.merge(\n",
    "            _df[\"tracks\"].explode(), left_index=True, right_index=True\n",
    "        )\n",
    "    )\n",
    "    .pipe(df_to_var, \"df_after_explode\")\n",
    "    # Make new 'tracks' field; strip ' ' empty space track names to '' empty string\n",
    "    .pipe(lambda _df: _df.assign(tracks=_df[\"tracks_y\"].str.strip()))\n",
    "    # Don't need these fields anymore\n",
    "    .drop(columns=[\"tracks_x\", \"tracks_y\"])\n",
    "    # Filter out empty string track names\n",
    "    .query(\"tracks!=''\")\n",
    "    .pipe(df_to_var, \"df_after_empty_track_name_filter\")\n",
    "    .reset_index(drop=True)\n",
    "    .loc[:, ['id', 'tracks']]\n",
    "    .reset_index()\n",
    "    .rename(columns={'id': 'album_row_id', \n",
    "                    'index': 'track_id',\n",
    "                    'tracks': 'track_name',\n",
    "                    }\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo joining track_level_df and clean_df\n",
    "(track_level_df\n",
    " .head(15)\n",
    " .merge(clean_df, left_on=['album_row_id'], right_on=['id'], how='inner')\n",
    " .loc[:, ['album_row_id', 'track_id', 'title', 'track_name']]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
